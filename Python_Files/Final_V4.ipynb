{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "from numpy import save\n",
    "from numpy import asarray\n",
    "from numpy import load\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "import statistics \n",
    "import random\n",
    "import progressbar\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, MaxPool2D, LeakyReLU\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "joint_name = ['HeadF', 'HeadB', 'HeadL', 'SpineF', 'SpineM', 'SpineL', \n",
    "            'Offset1', 'Offset2', 'HipL', 'HipR', 'ElbowL', 'ArmL', \n",
    "            'ShoulderL', 'ShoulderR', 'ElbowR', 'ArmR', 'KneeR', \n",
    "            'KneeL', 'ShinL', 'ShinR']\n",
    "\n",
    "joints_idx = [[1, 2], [2, 3], [1, 3], [2, 4], [1, 4], [3, 4], [4, 5], \n",
    "            [5, 6], [4, 7], [7, 8], [5, 8], [5, 7], [6, 8], [6, 9], \n",
    "            [6, 10], [11, 12], [4, 13], [4, 14], [11, 13], [12, 13], \n",
    "            [14, 15], [14, 16], [15, 16], [9, 18], [10, 17], [18, 19], \n",
    "            [17, 20]]\n",
    "folder = 'datasets/data_4/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# normalizes a matrix\n",
    "def normalize(matrix):\n",
    "    max_numb = max(matrix[~np.isnan(matrix)])\n",
    "    norm = matrix/max_numb\n",
    "    return np.array(norm)\n",
    "    # return matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "# Get the distance to every single point \n",
    "def getAllDistances(matFile, numb):\n",
    "    mat = matFile[numb]\n",
    "    mat = mat.T\n",
    "    dist = cdist(mat, mat, 'euclidean')\n",
    "    norm = normalize(dist)\n",
    "    max_dist = max(dist.flatten())\n",
    "    return norm, max_dist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Get the absolute height difference to every single point \n",
    "def getAllHeights(matFile, numb):\n",
    "    mat = matFile[numb]\n",
    "    z = mat[2]\n",
    "    reshaped = int(matFile.shape[2])\n",
    "    height = []\n",
    "\n",
    "    for i in range(reshaped):\n",
    "        for j in range(reshaped):\n",
    "            if np.nan in [z[i], z[j]]:\n",
    "                height.append(np.nan)\n",
    "            else:\n",
    "                height.append(np.abs(z[i]-z[j]))\n",
    "\n",
    "    # normalizes height data\n",
    "    height = normalize(np.array(height).reshape(reshaped, reshaped))\n",
    "    return height, max(height.flatten())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Get the angle to every single point \n",
    "def getAllAngles(matFile, numb):\n",
    "    mat = matFile[numb]\n",
    "    mat = mat.T\n",
    "    angle = cdist(mat, mat, 'cosine')\n",
    "    return normalize(angle), max(angle.flatten())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def getData(data, numb): \n",
    "    cnn_inputs = []\n",
    "    max_dist_list = []\n",
    "    max_height_list = []\n",
    "    max_angle_list = []\n",
    "\n",
    "    bar = progressbar.ProgressBar()\n",
    "\n",
    "    for i in bar(range(numb)):\n",
    "        dist, max_dist = getAllDistances(data, i) \n",
    "        height, max_height = getAllHeights(data, i)\n",
    "        angle, max_angle = getAllAngles(data, i)\n",
    "\n",
    "        # max_dist_list.append(max_dist)\n",
    "        # max_height_list.append(max_height)\n",
    "        # max_angle_list.append(max_angle)\n",
    "\n",
    "        for j in range(0, data.shape[2]):\n",
    "            temp = np.array([dist[j], height[j], angle[j]])\n",
    "            first = temp[:,0:3]\n",
    "            second = temp[:,3:20]\n",
    "            first = first [ :, first[0].argsort()]\n",
    "            second = second [ :, second[0].argsort()]\n",
    "            output = np.concatenate((first, second), axis =1)\n",
    "            cnn_inputs.append(output)\n",
    "\n",
    "            big = output[:,:13]\n",
    "            where_are_NaNs = np.isnan(big)\n",
    "            big[where_are_NaNs] = 0\n",
    "            max_dist_list.append(np.max(big[0]))\n",
    "            max_height_list.append(np.max(big[1]))\n",
    "            max_angle_list.append(np.max(big[2]))\n",
    "    avg_max_dist = statistics.median(max_dist_list)\n",
    "    avg_max_height = statistics.median(max_height_list)\n",
    "    avg_max_angle = statistics.median(max_angle_list)\n",
    "    avg_max = [avg_max_dist, avg_max_height, avg_max_angle]\n",
    "\n",
    "    cnn_inputs = np.array(cnn_inputs)[:,:,:13]\n",
    "    final = []\n",
    "    temp_bin = np.arange(20)/20\n",
    "    hist_bin = temp_bin.tolist() + [1]\n",
    "    bar = progressbar.ProgressBar()\n",
    "\n",
    "    for i in bar(range(len(cnn_inputs))):\n",
    "        for j in range(3):\n",
    "            norm = cnn_inputs[i][j]/avg_max[j]\n",
    "            hist = np.histogram(norm, bins=hist_bin)[0]\n",
    "            gauss = gaussian_filter1d([float(i) for i in hist], 1)\n",
    "            final.append(gauss)\n",
    "    \n",
    "    cnn_inputs = np.array(final).reshape((cnn_inputs.shape[0], 60))\n",
    "    where_are_NaNs = np.isnan(cnn_inputs)\n",
    "    cnn_inputs[where_are_NaNs] = 0\n",
    "    return cnn_inputs\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_file = loadmat('mat_files/bigSet1.mat')\n",
    "test_file = loadmat('mat_files/bigSet2.mat')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "train_data = train_file['bigSet1']\n",
    "test_data = test_file['bigSet2']\n",
    "train_labels = train_file['labels1']\n",
    "test_labels = test_file['labels2']\n",
    "# set numb of data used\n",
    "numb_train = len(train_data)\n",
    "numb_test = 1000\n",
    "\n",
    "# Take some frames from the list to use as trian and test data\n",
    "# index_train = np.linspace(0, len(train_labels), num = numb_train, endpoint=False).astype(int)\n",
    "index_test = np.linspace(0, len(test_labels), num = numb_test, endpoint=False).astype(int)\n",
    "# pre_train_data = train_data[index_train]\n",
    "# pre_train_labels = train_labels[index_train]\n",
    "pre_train_data = train_data\n",
    "pre_train_labels = train_labels\n",
    "pre_test_data = test_data[index_test]\n",
    "pre_test_labels = test_labels[index_test]\n",
    "\n",
    "print(\"Shape of pre_train_data:\", pre_train_data.shape)\n",
    "print(\"Shape of pre_test_data:\", pre_test_data.shape)\n",
    "print(\"Shape of pre_train_labels:\", pre_train_labels.shape)\n",
    "print(\"Shape of pre_test_labels:\", pre_test_labels.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of pre_train_data: (221147, 3, 20)\n",
      "Shape of pre_test_data: (1000, 3, 20)\n",
      "Shape of pre_train_labels: (221147, 20)\n",
      "Shape of pre_test_labels: (1000, 20)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# Get measurement data for every 3D point\n",
    "train_data = getData(pre_train_data, numb_train)\n",
    "test_data = getData(pre_test_data, numb_test)\n",
    "\n",
    "# Flatten the trian labels to fit dimentions of data\n",
    "train_labels = pre_train_labels.flatten()[0:(numb_train*20)]-1\n",
    "test_labels = pre_test_labels.flatten()[0:(numb_test*20)]-1\n",
    "\n",
    "print(\"Shape of train_data:\", train_data.shape)\n",
    "print(\"Shape of test_data:\", test_data.shape)\n",
    "print(\"Shape of train_labels:\", train_labels.shape)\n",
    "print(\"Shape of test_labels:\", test_labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  4% (9378 of 221147) |                  | Elapsed Time: 0:00:31 ETA:   0:11:12"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Get index where the data is all 0\n",
    "nans_train = np.sort(np.where(~train_data.any(axis=1))[0])[::-1]\n",
    "nans_test = np.sort(np.where(~test_data.any(axis=1))[0])[::-1]\n",
    "\n",
    "# Turn data into lists\n",
    "train_data_new = list(train_data)\n",
    "test_data_new = list(test_data)\n",
    "train_labels_new = list(train_labels)\n",
    "test_labels_new = list(test_labels)\n",
    "\n",
    "bar = progressbar.ProgressBar()\n",
    "# Remove the nan values \n",
    "for i in bar(nans_train):\n",
    "    train_data_new.pop(i)\n",
    "    train_labels_new.pop(i)\n",
    "bar = progressbar.ProgressBar()\n",
    "for i in bar(nans_test):\n",
    "    test_data_new.pop(i)\n",
    "    test_labels_new.pop(i)\n",
    "\n",
    "# Turn data back into array\n",
    "train_data_new = np.array(train_data_new)\n",
    "train_labels_new = np.array(train_labels_new)\n",
    "test_data_new = np.array(test_data_new)\n",
    "test_labels_new = np.array(test_labels_new)\n",
    "\n",
    "print(\"Shape of train_data_new:\", train_data_new.shape)\n",
    "print(\"Shape of test_data_new:\", test_data_new.shape)\n",
    "print(\"Shape of train_labels_new:\", train_labels_new.shape)\n",
    "print(\"Shape of test_labels_new:\", test_labels_new.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "- |#                                                  | 0 Elapsed Time: 0:00:00\n",
      "- |#                                                  | 0 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of train_data_new: (20000, 60)\n",
      "Shape of test_data_new: (20000, 60)\n",
      "Shape of train_labels_new: (20000,)\n",
      "Shape of test_labels_new: (20000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Creates the ML training platform to predict rat joints\n",
    "def ml_traning(train_data, train_labels, test_data, test_labels):\n",
    "    # DIMENTION CHANGE\n",
    "    # train_data = train_data.reshape(train_data.shape[0], 39)\n",
    "    # test_data = test_data.reshape(test_data.shape[0], 39)\n",
    "    train_data = train_data.reshape(train_data.shape[0], 60)\n",
    "    test_data = test_data.reshape(test_data.shape[0], 60)\n",
    "    train_data = train_data.astype('float32')\n",
    "    test_data = test_data.astype('float32')\n",
    "\n",
    "    model = createModel()\n",
    "    ml_folder = os.path.join(folder,\"training\")\n",
    "    if not os.path.isdir(ml_folder):\n",
    "        os.mkdir(ml_folder)\n",
    "    checkpoint_path = ml_folder + \"/cp.ckpt\"\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "    model.fit(x=train_data,y=train_labels, verbose='auto', batch_size=20, epochs=10, validation_data=(test_data, test_labels), callbacks=[cp_callback])\n",
    "    \n",
    "    loss, acc = model.evaluate(test_data, test_labels, verbose=1)\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Accuracy:\", acc*100)\n",
    "    return model\n",
    "\n",
    "# Creates the model for the CNN\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(640, activation= LeakyReLU()))\n",
    "    model.add(Dense(320, activation= LeakyReLU()))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Dense(80, activation= LeakyReLU()))\n",
    "    model.add(Dense(len(joint_name), activation = \"softmax\"))\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "save(folder + 'train_data.npy', asarray(train_data_new))\n",
    "save(folder + 'train_labels.npy', asarray(train_labels_new))\n",
    "save(folder + 'test_data.npy', asarray(test_data_new))\n",
    "save(folder + 'test_labels.npy', asarray(test_labels_new))\n",
    "\n",
    "# train_data_new = load(folder + 'train_data.npy')\n",
    "# train_labels_new = load(folder + 'train_labels.npy')\n",
    "# test_data_new = load(folder + 'test_data.npy')\n",
    "# test_labels_new = load(folder + 'test_labels.npy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "skeleton_model = ml_traning(train_data_new, train_labels_new, test_data_new, test_labels_new)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 8s 5ms/step - loss: 2.3386 - accuracy: 0.2910 - val_loss: 2.0318 - val_accuracy: 0.3794\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9831 - accuracy: 0.3954 - val_loss: 1.9137 - val_accuracy: 0.4139\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9063 - accuracy: 0.4132 - val_loss: 1.8735 - val_accuracy: 0.4291\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.8621 - accuracy: 0.4260 - val_loss: 1.8461 - val_accuracy: 0.4243\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.8255 - accuracy: 0.4380 - val_loss: 1.7929 - val_accuracy: 0.4474\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7887 - accuracy: 0.4479 - val_loss: 1.7594 - val_accuracy: 0.4548\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7558 - accuracy: 0.4555 - val_loss: 1.7241 - val_accuracy: 0.4629\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7223 - accuracy: 0.4665 - val_loss: 1.7244 - val_accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.6950 - accuracy: 0.4749 - val_loss: 1.6967 - val_accuracy: 0.4665\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.6670 - accuracy: 0.4846 - val_loss: 1.6654 - val_accuracy: 0.4732\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 1.6654 - accuracy: 0.4732\n",
      "Loss: 1.6654009819030762\n",
      "Accuracy: 47.31999933719635\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # skeleton_model = ml_traning(train_data_new, train_labels_new, test_data_new, test_labels_new)\n",
    "# checkpoint_path = \"training_2/cp.ckpt\"\n",
    "# temp_model = createModel()\n",
    "# temp_model.load_weights(checkpoint_path).expect_partial()\n",
    "\n",
    "# # Re-evaluate the model\n",
    "# loss, acc = temp_model.evaluate(test_data, test_labels, verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tests individual points and tells you if the CNN worked\n",
    "def handTest(image_index, test_data, label, model, maxNumb):\n",
    "    max_index = len(joint_name)*maxNumb-1\n",
    "    if image_index <= max_index:\n",
    "        predict(test_data, image_index, label, model)\n",
    "    else:\n",
    "        print(\"ERROR: Your predict_index must be below\", max_index)\n",
    "\n",
    "# Showing individual predictions\n",
    "def predict(data, image_index, label, model):\n",
    "    img = [data[image_index]]\n",
    "    # DIMENTION CHANGE\n",
    "    img = np.array(img).reshape(3,13)\n",
    "    # img = np.array(img).reshape(3,20)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    pred = model.predict(img.reshape(1, 39))\n",
    "    # pred = model.predict(img.reshape(1, 60))\n",
    "    predict = pred.argmax()\n",
    "    actual = label[image_index]\n",
    "\n",
    "    print(\"Actual:\", actual)\n",
    "    print(\"Predicted:\", predict)\n",
    "    \n",
    "    if predict == actual:\n",
    "        print(\"YAY ✿(ᵔ‿ᵔ)\")\n",
    "    else:\n",
    "        print(\"Wrong (◕︵◕)\")\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for i in range(420,440):\n",
    "#     handTest(i, test_data, test_labels, skeleton_model, numb_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# index = 1234\n",
    "# frame = int(index/20)\n",
    "\n",
    "# print(\"Train input\")\n",
    "# plt.imshow(pre_train_data[frame].reshape(3,20))\n",
    "# plt.show()\n",
    "# # DIMENTION CHANGE\n",
    "# plt.imshow(train_data[index].reshape(3,13))\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Test input\")\n",
    "# plt.imshow(pre_test_data[frame].reshape(3,20))\n",
    "# plt.show()\n",
    "# plt.imshow(test_data[index].reshape(3,13))\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "temporal = loadmat('mat_files/test_temp.mat')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temporal = loadmat('mat_files/test_temp.mat')\n",
    "time_pts = temporal['new_combed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# num_fms = 3000\n",
    "# num_fms = time_pts.shape[1]\n",
    "num_pts = int(time_pts.shape[0]/3) #22\n",
    "test_pts = time_pts[:,100000:106000]\n",
    "num_fms = np.array(test_pts).shape[1]\n",
    "\n",
    "plt.imshow(time_pts, interpolation='nearest', aspect=\"auto\")\n",
    "plt.show()\n",
    "# plt.imshow(test_pts, interpolation='nearest', aspect=\"auto\")\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(num_fms)\n",
    "input_pts = test_pts.reshape(num_pts, 3, num_fms).T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_time = getData(input_pts, len(input_pts))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "process_pts = processed_time.reshape(num_fms, 22, 39)\n",
    "temp = processed_time.reshape(num_fms, 22, 39)\n",
    "processed = np.transpose(temp,(1, 0, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # Check if process_pts is valid\n",
    "\n",
    "# frame_index = 2999\n",
    "# pt = 2\n",
    "# plt.imshow(input_pts[frame_index].reshape(3,num_pts))\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(process_pts[frame_index], interpolation='nearest', aspect=\"auto\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # plt.imshow([presplit_process[frame_index]], interpolation='nearest', aspect=\"auto\")\n",
    "# plt.imshow(processed[:,frame_index], interpolation='nearest', aspect=\"auto\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(process_pts[frame_index][pt].reshape(3,13))\n",
    "# plt.show()\n",
    "# plt.imshow(processed[pt][frame_index].reshape(3,13))\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chunks = []\n",
    "temp = np.ones((22,num_fms))\n",
    "bar = progressbar.ProgressBar()\n",
    "# run through all the registered points (0,22)\n",
    "for i in bar(range(num_pts)):\n",
    "    # get every third row\n",
    "    row = i*3\n",
    "    row_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    # run through length of each row (0,3000)\n",
    "    for j in range(num_fms): \n",
    "        # not a nan, add index to list\n",
    "        if not np.isnan(test_pts[row][j]):\n",
    "            temp_list.append(j)\n",
    "            temp[i][j] = temp[i][j]*2\n",
    "        # if element is a nan and the current list is not empty\n",
    "        elif len(temp_list) > 0:\n",
    "            # add list to output and clear the list\n",
    "            row_list.append([temp_list[0], temp_list[-1]])\n",
    "            temp_list = []\n",
    "    # edge case, add to output if the list still contains values\n",
    "    if len(temp_list) > 0:\n",
    "        row_list.append([temp_list[0], temp_list[-1]])\n",
    "    chunks.append(row_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plt.imshow(temp, interpolation='nearest', aspect=\"auto\")\n",
    "# plt.show\n",
    "# plt.imshow(test_pts, interpolation='nearest', aspect=\"auto\")\n",
    "# plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check the chunk list\n",
    "chunks = np.array(chunks, dtype=list)\n",
    "for i in range(len(chunks)):\n",
    "    print(\"Point: \"+str(i) +\", Chunks: \" + str(len(chunks[i])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy import stats\n",
    "temporal_data = []\n",
    "predict_numbs = []\n",
    "confidence_levels = []\n",
    "\n",
    "# loop through all the points (0,22)\n",
    "for i in range(len(chunks)):\n",
    "# for i in range(3):\n",
    "    seg_range = []\n",
    "    mode_predict = []\n",
    "    conf_predict = []\n",
    "    conf_level = []\n",
    "    pred_numbs = []\n",
    "\n",
    "\n",
    "    # list of chunks in a row\n",
    "    data = chunks[i]\n",
    "    print(\"\\nPoint: \"+ str(i) + \", Segments: \" + str(len(data)))\n",
    "    # loop through all the chunks per row (0,1)\n",
    "    for j in range(len(data)):\n",
    "        conf = []\n",
    "        pred = []\n",
    "        seg = data[j] #segment range ie. [0,2999]\n",
    "        lower = seg[0]\n",
    "        upper = seg[1]+1\n",
    "        seg_len = upper - lower\n",
    "        chunk_range = None \n",
    "        # loop through range of each chunk (0,3000)\n",
    "        if seg_len > 1000:\n",
    "            chunk_range = np.linspace(lower, upper, num = 1000, endpoint=False).astype(int)\n",
    "            seg_len = 1000\n",
    "        else: \n",
    "            chunk_range = np.arange(lower, upper)\n",
    "        for k in chunk_range: \n",
    "        # for k in range(100):\n",
    "            # if k < 5:\n",
    "            #     plt.imshow(processed[i][k].reshape(3,13))\n",
    "            #     plt.show()\n",
    "            model_pred = skeleton_model.predict(processed[i][k].reshape(1,39))\n",
    "\n",
    "            # get the confidence of predicition\n",
    "            if len(conf) == 0: \n",
    "                conf = model_pred[0]\n",
    "            else:\n",
    "                conf += model_pred[0]\n",
    "\n",
    "            # get prediction\n",
    "            pred.append(model_pred.argmax())\n",
    "            \n",
    "        # turn into arrays\n",
    "        conf = np.array(conf)\n",
    "        pred = np.array(pred)\n",
    "\n",
    "        # get mode predictions\n",
    "        mode_data = stats.mode(pred, axis = 0)\n",
    "        mode_pred = mode_data[0].flatten()[0]\n",
    "        mode_count = mode_data[1].flatten()[0]\n",
    "        mode_conf = mode_count/seg_len\n",
    "\n",
    "        # get confidence predictions\n",
    "        conf_pred = conf.argmax()\n",
    "        conf_count = np.sum(pred == conf_pred)\n",
    "        conf_conf = conf.max()/seg_len\n",
    "\n",
    "        # adding to output arrays\n",
    "        # seg_range.append(seg)\n",
    "        conf_level.append(conf)\n",
    "        pred_numbs.append(pred)\n",
    "        mode_predict.append([mode_pred, mode_conf, seg, (seg_len-mode_count), mode_count, seg_len])\n",
    "        conf_predict.append([conf_pred, conf_conf, seg, (seg_len-conf_count), conf_count, seg_len])\n",
    "\n",
    "        # Display current state\n",
    "        print(\"Segment: \"+str(j) + \")\", seg, \"Predicted point: (M:\" + str(mode_pred) + \", C:\" + str(conf_pred) + \"), [\" + str(mode_count) + \", \" + str(conf_count) + \"] /\", seg_len, end = \" \")\n",
    "        if (mode_pred == conf_pred):\n",
    "            print(\"Yay\")\n",
    "        else:\n",
    "            print(\"Conflict\")\n",
    "    \n",
    "    # adding to output arrays\n",
    "    confidence_levels.append(conf_level)\n",
    "    predict_numbs.append(pred_numbs)\n",
    "    temporal_data.append([mode_predict, conf_predict])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save(folder + 'temporal_data.npy', asarray(temporal_data, dtype=object))\n",
    "# save(folder + 'confidence_levels.npy', asarray(confidence_levels, dtype=object))\n",
    "# save(folder + 'predict_numbs.npy', asarray(predict_numbs, dtype=object))\n",
    "\n",
    "# temporal_data = load(folder + 'temporal_data.npy', allow_pickle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_new = np.ones((22,num_fms))\n",
    "for i in range(len(temporal_data)):\n",
    "    for j in range(len(temporal_data[i])):\n",
    "        for k in range(len(temporal_data[i][j])):\n",
    "            start = temporal_data[i][j][k][2][0]\n",
    "            end = temporal_data[i][j][k][2][1] + 1\n",
    "            for k in range(start, end):\n",
    "                temp_new[i][k] +=1\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.imshow(temp_new, interpolation='nearest', aspect=\"auto\")\n",
    "ax2.imshow(test_pts[::3], interpolation='nearest', aspect=\"auto\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_copy = temporal_data.copy()\n",
    "temp_data = np.array(temp_copy, dtype=object)\n",
    "# conf_copy = confidence_levels.copy()\n",
    "# conf_data = np.array(conf_copy, dtype=object)\n",
    "\n",
    "\n",
    "chunk_data = []\n",
    "chunk_conf = []\n",
    "pred_data = temp_data[:,1] # 1 for confidence\n",
    "# pred_data = temp_data[:,0] # 0 for mode\n",
    "print(pred_data.shape)\n",
    "\n",
    "for i in range(20):\n",
    "    chunk_data.append([])\n",
    "    # chunk_conf.append([])\n",
    "\n",
    "# putting the preditions into buckets\n",
    "for i in range(20):\n",
    "    index = pred_data[i]\n",
    "    # conf = conf_data[i]\n",
    "    for j in range(len(index)):\n",
    "        chunk_data[index[j][0]].append([i]+index[j])\n",
    "        # chunk_conf[index[j][0]].append([i]+conf[j])\n",
    "\n",
    "for i in range(len(chunk_data)):\n",
    "    print(i, len(chunk_data[i]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def squish(chunk_data):\n",
    "    processed_pts = []\n",
    "    unprocessed_pts = []\n",
    "\n",
    "    # squish all the data points\n",
    "    for i in range(20): \n",
    "        numb_index = []\n",
    "        unprocessed = []\n",
    "        bounds = None\n",
    "        current = chunk_data[i]\n",
    "        sorted_current = sorted(current,key=lambda x: x[4])\n",
    "\n",
    "        for j in reversed(range(len(sorted_current))):\n",
    "            input_index = sorted_current[j]\n",
    "            current_bounds = np.arange(input_index[3][0], input_index[3][1]+1).tolist()\n",
    "\n",
    "            if len(numb_index) == 0:\n",
    "                numb_index.append(input_index)\n",
    "                bounds = current_bounds\n",
    "            else:\n",
    "                same = list(set(current_bounds) & set(bounds)) # gets overlapping elements \n",
    "                # no overlapp between the two lists\n",
    "                if len(same) == 0:\n",
    "                    numb_index.append(input_index)\n",
    "                    bounds += current_bounds\n",
    "                else: #if there is overlap between the two lists\n",
    "                    if current_bounds[0] in bounds or current_bounds[-1] in bounds:\n",
    "                        # print(str(i) + \") Can't add index in range [\" + str(current_bounds[0]) + \", \" + str(current_bounds[-1]) + \"] to [\" + str(bounds[0])+ \", \" + str(bounds[-1]) + \"]\")\n",
    "                        unprocessed.append(input_index)\n",
    "                        pass\n",
    "                    elif current_bounds[0] in bounds:\n",
    "                        print(\"Lower bound inside current bounds\")\n",
    "                        pass\n",
    "                    elif current_bounds[-1] in bounds:\n",
    "                        print(\"Upper bound inside current bounds\")\n",
    "                        pass\n",
    "            bounds = sorted(bounds)\n",
    "        processed_pts.append(numb_index)\n",
    "        unprocessed_pts.append(unprocessed)\n",
    "    processed_pts = np.array(processed_pts, dtype = object)\n",
    "    unprocessed_pts = np.array(unprocessed_pts, dtype = object)\n",
    "    return processed_pts, unprocessed_pts\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_pts, unprocessed_pts = squish(chunk_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(unprocessed_pts[3][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for i in range(len(unprocessed_pts)):\n",
    "#     for j in range(len(unprocessed_pts[i])):\n",
    "#         print(unprocessed_pts[i][j])\n",
    "#         plt.imshow(confidence_levels[i][j])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(len(processed_pts)):\n",
    "    print(\"Point:\", i, end = \" --> \")\n",
    "    for j in range(len(processed_pts[i])):\n",
    "        print(processed_pts[i][j][0], end = \", \")\n",
    "    print(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_points = []\n",
    "point_coord = np.ones((60,num_fms))*np.nan\n",
    "\n",
    "for i in range(len(processed_pts)):\n",
    "    index = [i*3, i*3+1, i*3+2]\n",
    "    coord = [[],[],[]]\n",
    "\n",
    "    for j in range(len(processed_pts[i])):\n",
    "        seg_range = processed_pts[i][j]\n",
    "        org_index = seg_range[0]\n",
    "        start = seg_range[3][0]\n",
    "        end = seg_range[3][1]\n",
    "        # print(i,org_index,start,end)\n",
    "        index_xyz = [org_index*3, org_index*3+1, org_index*3+2]\n",
    "\n",
    "        for k in range(start, end):\n",
    "            point_coord[index[0]][k] = test_pts[index_xyz[0]][k]\n",
    "            point_coord[index[1]][k] = test_pts[index_xyz[1]][k]\n",
    "            point_coord[index[2]][k] = test_pts[index_xyz[2]][k]\n",
    "            if np.isnan(test_pts[index_xyz[0]][k]):\n",
    "                print(\"big error\", org_index, k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(point_coord, interpolation='nearest', aspect=\"auto\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Draw major lines in the rat model\n",
    "def drawLines(ax,x, y, z):\n",
    "    # run through all the connections to draw the points\n",
    "    for i in range(len(joints_idx)):\n",
    "        try: \n",
    "            # Getting both points to draw line\n",
    "            idx = joints_idx[i]\n",
    "            first_pt = idx[0]-1\n",
    "            second_pt = idx[1]-1\n",
    "            x_line = [x[first_pt], x[second_pt]]\n",
    "            y_line = [y[first_pt], y[second_pt]]\n",
    "            z_line = [z[first_pt], z[second_pt]]\n",
    "            # Draw lines\n",
    "            if i < 3: \n",
    "                ax.plot(x_line, y_line, z_line, c=\"#064ea1\", linewidth=4)\n",
    "            elif i < 6:\n",
    "                ax.plot(x_line, y_line, z_line, c=\"#64ccd1\", linewidth=4)\n",
    "            else:\n",
    "                ax.plot(x_line, y_line, z_line, c=\"#46b8a7\", linewidth=4)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Draw all line from every point \n",
    "def drawAllLines(ax,x, y, z):\n",
    "    joint_len = len(joint_name)\n",
    "    all_lines = []\n",
    "    for i in range(joint_len):\n",
    "        for j in range(joint_len):\n",
    "            if (not([i,j] in all_lines)):\n",
    "                x_line = [x[i], x[j]]\n",
    "                y_line = [y[i], y[j]]\n",
    "                z_line = [z[i], z[j]]\n",
    "                ax.plot(x_line, y_line, z_line, color='#b1d8fc', linewidth=0.5)\n",
    "                all_lines.append([i,j])\n",
    "                all_lines.append([j,i])\n",
    "\n",
    "def addLabels(ax, x, y, z):\n",
    "    # run through all the points to add labels\n",
    "    for i in range(len(x)):\n",
    "        try:\n",
    "            # points_array.append([x[i], y[i], z[i]])\n",
    "            # label = np.around(points_array[i]).astype(int)\n",
    "            label = joint_name[i]\n",
    "            if not np.isnan(x[i]):\n",
    "                ax.text(x[i], y[i], z[i], label)\n",
    "            # print(i, \":\", points_array[i], joint_name[i])\n",
    "        except:\n",
    "            pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %matplotlib tk\n",
    "\n",
    "# Plot 3D points given file and frame number\n",
    "def plotFrame(pts, frame, draw):\n",
    "    x = frame[0]\n",
    "    y = frame[1]\n",
    "    z = frame[2]\n",
    "    x_pts = pts[0]\n",
    "    y_pts = pts[1]\n",
    "    z_pts = pts[2]\n",
    "\n",
    "    # plt.imshow([x, y,z])\n",
    "    # plt.show()\n",
    "\n",
    "    # loading plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    # adding points, labels and lines\n",
    "    try:\n",
    "        ax.scatter(x,y,z, color='#ff5e5e', s =10, marker='x')\n",
    "        ax.scatter(x_pts,y_pts,z_pts, color='b', s =10,)\n",
    "    except:\n",
    "        pass\n",
    "    if draw == 0:\n",
    "        drawLines(ax, x, y, z)\n",
    "        addLabels(ax, x, y, z)\n",
    "    # drawAllLines(ax, x, y, z)\n",
    "\n",
    "    # Labeling plot\n",
    "    ax.set_title(\"Rat positioning\")\n",
    "    ax.set_xlabel(\"X axis\")\n",
    "    ax.set_ylabel(\"Y axis\")\n",
    "    ax.set_zlabel(\"Z axis\")\n",
    "    # rotate the axes and update\n",
    "    for angle in range(0, 360):\n",
    "        ax.view_init(30, angle)\n",
    "        plt.draw()\n",
    "        plt.pause(.001)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp = point_coord.copy()\n",
    "temp_2 = np.transpose(temp.T.reshape(num_fms, 20, 3), (0,2,1))\n",
    "temp_3 = test_pts.copy()\n",
    "temp_4 = np.transpose(temp_3.T.reshape(num_fms, 22, 3), (0,2,1))\n",
    "\n",
    "plt.imshow(temp_4[2500])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib qt\n",
    "plotFrame(temp_4[2500], temp_2[2500], 0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8de543a527d38bc21d309dfa8a60aed366b41867ed03b1c23b526625b89003b8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}