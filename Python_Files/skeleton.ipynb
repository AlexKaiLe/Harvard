{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexander Le's 2021 REU Harvard System Biology Internship\n",
    "## Sponsored by:\n",
    "<ul> \n",
    "<li>Harvard Medical School: Blavatnik Institute</li>\n",
    "<li>National Science Foundation</li>\n",
    "<li>Simons Foundation</li>\n",
    "<li>Harvard QBio</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "### Principal Investigator: Dr. Cengiz Pehlevan\n",
    "### Primary Mentor: Ugne Kilbaite\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We do not know exactly how the nervous system controls motor behavior in health or disease. Many scientific studies designed to reveal the brain areas, circuits, and chemistry are first conducted in rats, before being conducted in humans. Currently, it is extraordinarily time-intensive to record, interpret, and analyze rat movements 24/7, making it difficult if not impossible to relate these behaviors to neural activity. Therefore I am trying to solve this problem using computational techniques. My current internship project in the Pehlevan Lab is to develop a method for modeling the skeleton using a video feed of animal behavior. There are two major barriers to achieving this goal: (1) we do not always know which 3D data point on the external body correlates to which internal joint; and (2) some data points may not be picked up by the cameras due to environmental noise. Thus, we developing a machine-learning algorithm to predict the name of a 3D data point as well as predict the 3D coordinate of missing points. This algorithm consists of 3 major components: a convolutional neural network (CNN) for initial prediction of points, a graph neural network (GNN) for temporal prediction of points, and a variational autoencoder (VAE) to predict the coordinates of missing points. The code in this notebook consists of the CNN used to predict existing points. Although there is existing software that can help determine the 3D points in space, the code is not optimized for rat behavior and does not account for missing points. Furthermore, we are unable to modify the code to fit different needs since it is proprietary. My research will develop an accessible tool that will enable the analysis of real-time rat behavior without time-intensive user input. We are in a good position to develop this program because we have access to large amounts of rat movement recordings and a robust computing system that can process this large data. As a consequence of our work, many labs will also be able to more powerfully and efficiently determine how mutations, neurological conditions, environmental exposures, or medical interventions affect rat movement and behavior, generating insights and strategies for improving human health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Introduction\n",
    "\n",
    "The following code constructs a CNN that can predict the identity of a point in a given frame. The following notebook is divided into two major sections. The first section is to create a CNN to train and test a deep learning model using TensorFlow. The second section is to take temporal data and predict a cluster of points in time. At the end of the program, you can visualize a rat's movement in 3D space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import necessary libraries and modulues for the program to run'''\n",
    "\n",
    "import progressbar\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from statistics import median\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize global variables'''\n",
    "\n",
    "# list of joint names\n",
    "joint_name = ['HeadF', 'HeadB', 'HeadL', 'SpineF', 'SpineM', 'SpineL', \n",
    "            'Offset1', 'Offset2', 'HipL', 'HipR', 'ElbowL', 'ArmL', \n",
    "            'ShoulderL', 'ShoulderR', 'ElbowR', 'ArmR', 'KneeR', \n",
    "            'KneeL', 'ShinL', 'ShinR']\n",
    "\n",
    "# list of joint connections\n",
    "joints_idx = [[1, 2], [2, 3], [1, 3], [2, 4], [1, 4], [3, 4], [4, 5], \n",
    "            [5, 6], [4, 7], [7, 8], [5, 8], [5, 7], [6, 8], [6, 9], \n",
    "            [6, 10], [11, 12], [4, 13], [4, 14], [11, 13], [12, 13], \n",
    "            [14, 15], [14, 16], [15, 16], [9, 18], [10, 17], [18, 19], \n",
    "            [17, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create folders to save machine learning model'''\n",
    "\n",
    "def createSaveFolder():\n",
    "    '''\n",
    "    input: None\n",
    "    output: Directory where the model is saved in\n",
    "    '''\n",
    "    \n",
    "    if not os.path.isdir('datasets'):\n",
    "        os.mkdir('datasets')\n",
    "    save_folder = 'datasets/skeleton/'\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "    return save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Normalizes a matrix by dividing by the largest values in the array'''\n",
    "\n",
    "def normalize(matrix):\n",
    "    '''\n",
    "    Input: 2D array of features derived from the raw 3D points\n",
    "    Output: Normalized 2D array where the max number is 1\n",
    "    '''\n",
    "    \n",
    "    max_numb = max(matrix[~np.isnan(matrix)])\n",
    "    norm = matrix/max_numb\n",
    "    return np.array(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Determine the distance from a single point to every other point in a frame''' \n",
    "\n",
    "def getAllDistances(matFile, numb):\n",
    "    '''\n",
    "    Input:\n",
    "        matFile: 2D array of raw coordinate data, [x coordinates, y coordinates, z coordinates]\n",
    "        numb: Int of the frame we want to look at\n",
    "    Output: \n",
    "        angle: 2D array of raw distances\n",
    "        norm: 2D array of normalized distances\n",
    "        max_angle: int of max distances\n",
    "    '''\n",
    "    \n",
    "    mat = matFile[numb]\n",
    "    mat = mat.T\n",
    "    dist = cdist(mat, mat, 'euclidean')\n",
    "    norm = normalize(dist)\n",
    "    max_dist = max(dist.flatten())\n",
    "    return dist, norm, max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Determine the absolute height difference from a single point to every other point in a frame''' \n",
    "\n",
    "def getAllHeights(matFile, numb):\n",
    "    '''\n",
    "    Input:\n",
    "        matFile: 2D array of raw coordinate data, [x coordinates, y coordinates, z coordinates]\n",
    "        numb: Int of the frame we want to look at\n",
    "    Output: \n",
    "        angle: 2D array of raw heights\n",
    "        norm: 2D array of normalized heights\n",
    "        max_angle: int of max height\n",
    "    '''\n",
    "\n",
    "    mat = matFile[numb]\n",
    "    # get z coordinates\n",
    "    z = mat[2] \n",
    "    num_pts = int(matFile.shape[2])\n",
    "    height_diff = []\n",
    "\n",
    "    for i in range(num_pts):\n",
    "        for j in range(num_pts):\n",
    "            if np.nan in [z[i], z[j]]:\n",
    "                height_diff.append(np.nan)\n",
    "            else:\n",
    "                height_diff.append(np.abs(z[i]-z[j]))\n",
    "\n",
    "    height = np.array(height_diff).reshape(num_pts, num_pts)\n",
    "    norm = normalize(height)\n",
    "    max_height = max(height.flatten())\n",
    "    return height, norm, max_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Determine the angle from a single point to every other point in a frame'''\n",
    "\n",
    "def getAllAngles(matFile, numb):\n",
    "    '''\n",
    "    Input:\n",
    "        matFile: 2D array of raw coordinate data, [x coordinates, y coordinates, z coordinates]\n",
    "        numb: Int of the frame we want to look at\n",
    "    Output: \n",
    "        angle: 2D array of raw angles\n",
    "        norm: 2D array of normalized angles\n",
    "        max_angle: int of max angle\n",
    "    '''\n",
    "\n",
    "    mat = matFile[numb]\n",
    "    mat = mat.T\n",
    "    angle = cdist(mat, mat, 'cosine')\n",
    "    norm = normalize(angle)\n",
    "    max_angle = max(angle.flatten())\n",
    "    return angle, norm, max_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that obtains all the metrics from the raw 3D data and creates CNN inputs for the ML model to train/test'''\n",
    "\n",
    "def pureData(data, numb): \n",
    "    '''\n",
    "    Input:\n",
    "        data: Raw 3D array of raw coordinate data, total frames X [x coordinates, y coordinates, z coordinates]\n",
    "        numb: Int of the frames we want to look at\n",
    "    Output: \n",
    "        pureData: 2D array of CNN inputs for the ML model\n",
    "    '''\n",
    "\n",
    "    cnn_inputs = []\n",
    "    max_dist_list = []\n",
    "    max_height_list = []\n",
    "    max_angle_list = []\n",
    "\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for i in bar(range(numb)):\n",
    "        # Calculate Metrics from methods created above\n",
    "        dist, norm_dist, max_dist = getAllDistances(data, i) \n",
    "        height, norm_height, max_height = getAllHeights(data, i)\n",
    "        angle, norm_angle, max_angle = getAllAngles(data, i)\n",
    "\n",
    "        # Run through all the raw inputs and create a metric for every single point \n",
    "        # consisting of distance, height and angle measurements to every other point\n",
    "        for j in range(0, data.shape[2]):\n",
    "            temp = np.array([dist[j], height[j], angle[j]])\n",
    "            first = temp[:,0:3]\n",
    "            second = temp[:,3:20]\n",
    "            # sort the data based on the distance meteric \n",
    "            first = first [ :, first[0].argsort()]\n",
    "            second = second [ :, second[0].argsort()]\n",
    "            # Concatenate the three head points with the 10 closest points\n",
    "            output = np.concatenate((first, second), axis =1)\n",
    "            cnn_inputs.append(output)\n",
    "\n",
    "            # Find the largest distance, height and angle from the 13 input points\n",
    "            big = output[:,:13]\n",
    "            where_are_NaNs = np.isnan(big)\n",
    "            big[where_are_NaNs] = 0\n",
    "            max_dist_list.append(np.max(big[0]))\n",
    "            max_height_list.append(np.max(big[1]))\n",
    "            max_angle_list.append(np.max(big[2]))\n",
    "\n",
    "    # get the median max distance, height and angle from all frames\n",
    "    avg_max_dist = median(max_dist_list)\n",
    "    avg_max_height = median(max_height_list)\n",
    "    avg_max_angle = median(max_angle_list)\n",
    "    avg_max = [avg_max_dist, avg_max_height, avg_max_angle]\n",
    "\n",
    "    # normalize all the meterics by the median max distance, height and angle\n",
    "    cnn_inputs = np.array(cnn_inputs)[:,:,:13]\n",
    "    final = []\n",
    "    for i in range(len(cnn_inputs)):\n",
    "        for j in range(3):\n",
    "            final.append(cnn_inputs[i][j]/avg_max[j])\n",
    "    \n",
    "    # reshapes the inputs to a 2D array \n",
    "    cnn_inputs = np.array(final).reshape((cnn_inputs.shape[0], 39))\n",
    "\n",
    "    # remove all the nan values and set them to 0\n",
    "    where_are_NaNs = np.isnan(cnn_inputs)\n",
    "    cnn_inputs[where_are_NaNs] = 0\n",
    "    return cnn_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that can be manipulated to act as a switch to test diffent ways to obtain metrics and improve accuracy'''\n",
    "\n",
    "def getData(data, numb):\n",
    "    '''\n",
    "    Input:\n",
    "        data: Raw 3D array of raw coordinate data, total frames X [x coordinates, y coordinates, z coordinates]\n",
    "        numb: Int of the frames we want to look at\n",
    "    Output: \n",
    "        pureData: 2D array of CNN inputs for the ML model\n",
    "    '''\n",
    "    return pureData(data, numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that creates the train data/labels and test data/labels'''\n",
    "\n",
    "def trainTest(train_data, test_data, train_labels, test_labels, numb_train, numb_test):\n",
    "    '''\n",
    "    Input:\n",
    "        train_data: 3D array of raw coordinate data, total frames X [x coordinates, y coordinates, z coordinates]\n",
    "        test_data: 3D array of raw coordinate data, total frames X [x coordinates, y coordinates, z coordinates]\n",
    "        train_labels: 2D array of raw labels, frames X [label int position]\n",
    "        test_labels: 2D array of raw labels, frames X [label int position]\n",
    "        numb_train: Number of training data points\n",
    "        numb_test: Number of testing data points\n",
    "    Output: \n",
    "        train_data_new: 2D array of processed data, total points X [calculated meterics]\n",
    "        test_data_new: 2D array of processed data, total points X [calculated meterics]\n",
    "        train_labels_new: array of processed labels, total points X [labels]\n",
    "        test_labels_new: array of processed labels, total points X [labels]\n",
    "    '''\n",
    "\n",
    "    # create training data/labels\n",
    "    pre_train_data = train_data\n",
    "    pre_train_labels = train_labels\n",
    "    if numb_train != len(train_data):\n",
    "        index_train = np.linspace(0, len(train_labels), num = numb_train, endpoint=False).astype(int)\n",
    "        pre_train_data = train_data[index_train]\n",
    "        pre_train_labels = train_labels[index_train]\n",
    "\n",
    "    # create testing data/labels\n",
    "    index_test = np.linspace(0, len(test_labels), num = numb_test, endpoint=False).astype(int)\n",
    "    pre_test_data = test_data[index_test]\n",
    "    pre_test_labels = test_labels[index_test]\n",
    "\n",
    "    # Get meteric data for every point\n",
    "    print(\"Getting messurements\")\n",
    "    train_data = getData(pre_train_data, numb_train)\n",
    "    test_data = getData(pre_test_data, numb_test)\n",
    "\n",
    "    # Flatten the labels to fit dimentions of data\n",
    "    train_labels = pre_train_labels.flatten()[0:(numb_train*20)]-1\n",
    "    test_labels = pre_test_labels.flatten()[0:(numb_test*20)]-1\n",
    "\n",
    "    # Get index where the data is all 0\n",
    "    nans_train = np.sort(np.where(~train_data.any(axis=1))[0])[::-1]\n",
    "    nans_test = np.sort(np.where(~test_data.any(axis=1))[0])[::-1]\n",
    "    # Turn data into lists\n",
    "    train_data_new = list(train_data)\n",
    "    test_data_new = list(test_data)\n",
    "    train_labels_new = list(train_labels)\n",
    "    test_labels_new = list(test_labels)\n",
    "\n",
    "    # Remove all the nan values to show acutal accuracy\n",
    "    print(\"Removing nans\")\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for i in bar(nans_train):\n",
    "        train_data_new.pop(i)\n",
    "        train_labels_new.pop(i)\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for i in bar(nans_test):\n",
    "        test_data_new.pop(i)\n",
    "        test_labels_new.pop(i)\n",
    "\n",
    "    # Turn data back into np arrays\n",
    "    train_data_new = np.array(train_data_new)\n",
    "    train_labels_new = np.array(train_labels_new)\n",
    "    test_data_new = np.array(test_data_new)\n",
    "    test_labels_new = np.array(test_labels_new)\n",
    "    return train_data_new, test_data_new, train_labels_new, test_labels_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating machine learning input data\n",
    "\n",
    "To create the machine learning input data, we have to first load the raw data and process them by running it through the funcitons initialized above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This method loads the raw data and saves/loads data and labels for the machine learning model'''\n",
    "\n",
    "def ML_Data(create_data, save_files, save_folder):\n",
    "    '''\n",
    "    Input:\n",
    "        create_data: Boolean where True if we want to create new data \n",
    "        save_files: Boolean where True if we want to save files\n",
    "        save_folder: folder to save the files in \n",
    "    Output:\n",
    "        train_data_new = 2D array of processed data, total points X [calculated meterics]\n",
    "        test_data_new = 2D array of processed data, total points X [calculated meterics]\n",
    "        train_labels_new = array of processed labels, total points X [labels]\n",
    "        test_labels_new = array of processed labels, total points X [labels]\n",
    "        numb_test = int of points being tested\n",
    "    '''\n",
    "\n",
    "    # create variables \n",
    "    train_data_new = None\n",
    "    test_data_new = None\n",
    "    train_labels_new = None\n",
    "    test_labels_new = None\n",
    "\n",
    "    if create_data:\n",
    "        # load data\n",
    "        train_file = loadmat('mat_files/bigSet1.mat')\n",
    "        test_file = loadmat('mat_files/bigSet2.mat')\n",
    "        train_data = train_file['bigSet1']\n",
    "        test_data = test_file['bigSet2']\n",
    "        train_labels = train_file['labels1']\n",
    "        test_labels = test_file['labels2']\n",
    "\n",
    "        # set numb of data used\n",
    "        numb_train = len(train_data)\n",
    "        numb_test = 10000\n",
    "\n",
    "        # get data from trainTest method\n",
    "        train_data_new, test_data_new, train_labels_new, test_labels_new = trainTest(train_data, test_data, train_labels, test_labels, numb_train, numb_test)\n",
    "        \n",
    "        # save the data as .npy files\n",
    "        if (save_files): \n",
    "            print(\"Saving to:\", save_folder)\n",
    "            np.save(save_folder + 'train_data.npy', np.asarray(train_data_new))\n",
    "            np.save(save_folder + 'train_labels.npy', np.asarray(train_labels_new))\n",
    "            np.save(save_folder + 'test_data.npy', np.asarray(test_data_new))\n",
    "            np.save(save_folder + 'test_labels.npy', np.asarray(test_labels_new))\n",
    "        return train_data_new, test_data_new, train_labels_new, test_labels_new, numb_test\n",
    "    else:\n",
    "        # load saved files\n",
    "        try:\n",
    "            train_data_new = np.load(save_folder + 'train_data.npy')\n",
    "            train_labels_new = np.load(save_folder + 'train_labels.npy')\n",
    "            test_data_new = np.load(save_folder + 'test_data.npy')\n",
    "            test_labels_new = np.load(save_folder + 'test_labels.npy')\n",
    "            numb_test = len(test_labels_new)\n",
    "            return train_data_new, test_data_new, train_labels_new, test_labels_new, numb_test\n",
    "        except:\n",
    "            print(\"There are no files to be opened\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creates the ML training model to predict rat joints'''\n",
    "\n",
    "def ml_training(train_data, train_labels, test_data, test_labels, save, save_folder):\n",
    "    '''\n",
    "    Input: \n",
    "        train_data: 2D array of processed data, total points X [calculated meterics]\n",
    "        train_labels: array of processed labels, total points X [labels]\n",
    "        test_data: 2D array of processed data, total points X [calculated meterics]\n",
    "        test_labels: array of processed labels, total points X [labels]\n",
    "        save: Boolean where True if we want to save ML model\n",
    "    Output: \n",
    "        ml_model: Sequential ML model\n",
    "    '''\n",
    "\n",
    "    # Reshape and format data\n",
    "    ml_train_data = train_data.reshape(train_data.shape[0], 39)\n",
    "    ml_test_data = test_data.reshape(test_data.shape[0], 39)\n",
    "    ml_train_data = ml_train_data.astype('float32')\n",
    "    ml_test_data = ml_test_data.astype('float32')\n",
    "    \n",
    "    # create ML model\n",
    "    ml_model = createModel()\n",
    "\n",
    "    # Train model\n",
    "    if save:\n",
    "        ml_folder = os.path.join(save_folder, \"training\")\n",
    "        if not os.path.isdir(ml_folder):\n",
    "            os.mkdir(ml_folder)\n",
    "        checkpoint_path = ml_folder + \"/cp.ckpt\"\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "        ml_model.fit(x=ml_train_data,y=train_labels, verbose='auto', batch_size=100, epochs=5, validation_data=(ml_test_data, test_labels), callbacks=[cp_callback])\n",
    "    else: \n",
    "        ml_model.fit(x=ml_train_data,y=train_labels, verbose='auto', batch_size=100, epochs=5, validation_data=(ml_test_data, test_labels))\n",
    "    \n",
    "    # Print loss and accuracy \n",
    "    loss, acc = ml_model.evaluate(ml_test_data, test_labels, verbose=1)\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Accuracy:\", acc*100)\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creates the model for the CNN'''\n",
    "\n",
    "def createModel():\n",
    "    '''\n",
    "    Input: None\n",
    "    Ouptut: \n",
    "        model: Sequential ML model\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(640, activation= LeakyReLU()))\n",
    "    model.add(Dense(640, activation= LeakyReLU()))\n",
    "    model.add(Dense(320, activation= LeakyReLU()))\n",
    "    model.add(Dense(80, activation= LeakyReLU()))\n",
    "    model.add(Dense(len(joint_name), activation = \"softmax\"))\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''If we already trained a model, load the pretrained model to use'''\n",
    "\n",
    "def getStoredModel():\n",
    "    '''\n",
    "    Input: None\n",
    "    Output:  \n",
    "        skeleton_model: Sequential ML model \n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Obtain file with saved ML model\n",
    "        checkpoint_path = save_folder + \"training/cp.ckpt\"\n",
    "        skeleton_model = createModel()\n",
    "        skeleton_model.load_weights(checkpoint_path).expect_partial()\n",
    "\n",
    "        # Evaluate the model\n",
    "        loss, acc = skeleton_model.evaluate(test_data_new, test_labels_new)\n",
    "        print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "        return skeleton_model\n",
    "    except:\n",
    "        print(\"There is no stored ML file\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create confusion matrix of ML model'''\n",
    "\n",
    "def dispConfMat(skeleton_model):\n",
    "    '''\n",
    "    Input:\n",
    "        skeleton_model: Sequential ML model \n",
    "    Output: None\n",
    "    '''\n",
    "\n",
    "    # get confusion matrix\n",
    "    test_pred = np.argmax(skeleton_model.predict(test_data_new), axis=-1)\n",
    "    con_mat = tf.math.confusion_matrix(labels=test_labels_new, predictions=test_pred).numpy()\n",
    "    row_sums = con_mat.sum(axis=1)\n",
    "    new_matrix = con_mat / row_sums[:, np.newaxis]\n",
    "\n",
    "    # plot confusion matrix and save it as tiff\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xticks((0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19))\n",
    "    ax.set_xticklabels(joint_name, rotation=90)\n",
    "    ax.set_yticks((0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19))\n",
    "    ax.set_yticklabels(joint_name)\n",
    "    ax.xaxis.tick_top()\n",
    "    temp = ax.imshow(new_matrix,interpolation='nearest', aspect=1)\n",
    "    plt.colorbar(temp, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"cm.tiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains or obtains pre-trained ML model'''\n",
    "\n",
    "def get_model(train_data_new, train_labels_new, test_data_new, test_labels_new, save_folder, train):\n",
    "    '''\n",
    "    Input: \n",
    "        train_data_new = 2D array of processed data, total points X [calculated meterics]\n",
    "        train_labels_new = array of processed labels, total points X [labels]\n",
    "        test_data_new = 2D array of processed data, total points X [calculated meterics]\n",
    "        test_labels_new = array of processed labels, total points X [labels] \n",
    "        save_folder: folder to save ML model\n",
    "        train: Boolean where True if we want to train or load ML model \n",
    "    Output:\n",
    "    '''\n",
    "\n",
    "    skeleton_model = None\n",
    "    if train:\n",
    "        skeleton_model = ml_training(train_data_new, train_labels_new, test_data_new, test_labels_new, True, save_folder)\n",
    "    else:\n",
    "        skeleton_model = getStoredModel()\n",
    "    \n",
    "    # show confusion matrix \n",
    "    dispConfMat(skeleton_model)\n",
    "    return skeleton_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tests individual points and tells you if the CNN worked'''\n",
    "\n",
    "def handTest(image_index, test_data, label, model, maxNumb):\n",
    "    '''\n",
    "    Input:\n",
    "        image_index: int of point you want to predict\n",
    "        test_data: 2D array \n",
    "        label:\n",
    "        model:\n",
    "        maxNumb:\n",
    "    Output:\n",
    "    '''\n",
    "    if image_index <= maxNumb:\n",
    "        predict(test_data, image_index, label, model)\n",
    "    else:\n",
    "        print(\"ERROR: Your predict_index must be below\", maxNumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing individual predictions\n",
    "def predict(data, image_index, label, model):\n",
    "    img = data[image_index]\n",
    "    length_flat = len(img)\n",
    "    length_img = int(length_flat/3)\n",
    "    img = np.array(img).reshape(3,length_img)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Meterics for head points and 10 closest points\")\n",
    "    plt.xlabel(\"Points in frame\")\n",
    "    plt.ylabel(\"Height, Angle, Distance \")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    pred = model.predict(img.reshape(1, length_flat))\n",
    "    predict = pred.argmax()\n",
    "    actual = label[image_index]\n",
    "\n",
    "    print(\"Actual:\", actual)\n",
    "    print(\"Predicted:\", predict)\n",
    "    \n",
    "    if predict == actual:\n",
    "        print(\"YAY ✿(ᵔ‿ᵔ)\")\n",
    "    else:\n",
    "        print(\"Wrong (◕︵◕)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save data\n",
    "save_folder = createSaveFolder()\n",
    "\n",
    "# Get data for ML training \n",
    "create_data = True\n",
    "save_files = True\n",
    "train_data_new, test_data_new, train_labels_new, test_labels_new, numb_test = ML_Data(create_data, save_files, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_model = get_model(train_data_new, train_labels_new, test_data_new, test_labels_new, save_folder, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(420,425):\n",
    "    handTest(i, test_data_new, test_labels_new, skeleton_model, numb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loads in temporal data from mat or h5py files and formats in a readable context'''\n",
    "\n",
    "def getTemporalData(mat_file):\n",
    "    '''\n",
    "    Input:\n",
    "        mat_file: Boolean where True opens a mat file and False opens a h5py file\n",
    "    Output:\n",
    "        new_temp_data: 2D array of temporal data\n",
    "        new_temp_labels: 2D array of temporal labels\n",
    "        num_fms: int number of frames\n",
    "        num_pts: int number of pts\n",
    "    '''\n",
    "    \n",
    "    copy_data = None \n",
    "    copy_labels = None\n",
    "\n",
    "    if mat_file:\n",
    "        # loads a temporal mat file \n",
    "        temporal_file = loadmat('mat_files/bigSet3.mat')\n",
    "        temp_data = temporal_file['bigSet3']\n",
    "        temp_labels = temporal_file['labels3']\n",
    "        copy_data = temp_data.copy()\n",
    "        copy_labels = temp_labels.copy()\n",
    "        copy_data = np.transpose(copy_data,(2,1,0))\n",
    "        copy_labels = copy_labels.T\n",
    "    else: \n",
    "        # loads a temporal h5py file \n",
    "        temporal_file = h5py.File('mat_files/JDM25baseline_Ext4_timematched_controltsne_analysis_struct_mocapdata.mat', 'r') \n",
    "        temp_data = temporal_file['alteredM'][:]\n",
    "        temp_labels = temporal_file['labelsM'][:]\n",
    "        temporal_file.close()\n",
    "        copy_data = temp_data.copy()\n",
    "        copy_labels = temp_labels.copy()\n",
    "        \n",
    "    new_temp_data = copy_data[:,:,:10000]\n",
    "    num_fms =  new_temp_data.shape[2]\n",
    "    num_pts = len(new_temp_data)\n",
    "    new_temp_labels = copy_labels[:,:num_fms]\n",
    "    \n",
    "    return new_temp_data, new_temp_labels, num_fms, num_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTempData(new_temp_data, new_temp_labels):\n",
    "    time_pts = []\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for i in bar(range(new_temp_labels.shape[1])):\n",
    "        label = np.array(new_temp_labels[:,i])\n",
    "        data = new_temp_data[:,:,i]\n",
    "        stack = np.column_stack((data, label))\n",
    "        stack = np.array(sorted(stack,key=lambda x: x[3]))\n",
    "        time_pts.append(stack[:,:3])\n",
    "        \n",
    "    time_pts = np.array(time_pts).reshape(num_fms, 3*num_pts).T\n",
    "    plt.imshow(time_pts, interpolation='nearest', aspect=\"auto\")\n",
    "    plt.title(\"Full segments\")\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"XYZ points\")\n",
    "    plt.show()\n",
    "    return time_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = True\n",
    "new_temp_data, new_temp_labels, num_fms, num_pts = getTemporalData(mat_file)\n",
    "test_pts = processTempData(new_temp_data, new_temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeChunks(test_pts):\n",
    "    time_pts = test_pts.copy()\n",
    "    for i in range(3,num_pts):\n",
    "        times = random.randint(2,4)\n",
    "        for j in range(times):\n",
    "            length_gap = random.randint(600, 2000)\n",
    "            start_gap = random.randint(0,num_fms-1)\n",
    "            end_gap = start_gap + length_gap\n",
    "            time_pts[i*3][start_gap:end_gap] = np.nan\n",
    "            time_pts[i*3+1][start_gap:end_gap] = np.nan\n",
    "            time_pts[i*3+2][start_gap:end_gap] = np.nan\n",
    "    plt.imshow(time_pts, interpolation='nearest', aspect=\"auto\")\n",
    "    plt.title(\"Full segments\")\n",
    "    plt.xlabel(\"Time (frames)\")\n",
    "    plt.ylabel(\"XYZ points\")\n",
    "    plt.show()\n",
    "    return time_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pts = removeChunks(test_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPoints(time_pts):\n",
    "    input_pts = time_pts.reshape(num_pts, 3, num_fms).T\n",
    "    processed_time = getData(input_pts, len(input_pts))\n",
    "    process_pts = processed_time.reshape(num_fms, num_pts, 39)\n",
    "    processed = np.transpose(process_pts,(1, 0, 2))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = processPoints(time_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunks(num_pts, num_fms, time_pts):\n",
    "    chunks = []\n",
    "    temp = np.ones((num_pts,num_fms))\n",
    "    bar = progressbar.ProgressBar()\n",
    "    # run through all the registered points (0,22)\n",
    "    for i in bar(range(num_pts)):\n",
    "        # get every third row\n",
    "        row = i*3\n",
    "        row_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        # run through length of each row (0,3000)\n",
    "        for j in range(num_fms): \n",
    "            # not a nan, add index to list\n",
    "            if not np.isnan(time_pts[row][j]):\n",
    "                temp_list.append(j)\n",
    "                temp[i][j] = temp[i][j]*2\n",
    "            # if element is a nan and the current list is not empty\n",
    "            elif len(temp_list) > 0:\n",
    "                # add list to output and clear the list\n",
    "                row_list.append([temp_list[0], temp_list[-1]])\n",
    "                temp_list = []\n",
    "        # edge case, add to output if the list still contains values\n",
    "        if len(temp_list) > 0:\n",
    "            row_list.append([temp_list[0], temp_list[-1]])\n",
    "        chunks.append(row_list)\n",
    "\n",
    "    chunks = np.array(chunks, dtype=list)\n",
    "    for i in range(len(chunks)):\n",
    "        print(\"Point: \"+str(i) +\", Segments: \" + str(len(chunks[i])))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = getChunks(num_pts, num_fms, time_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTemporalData(chunks, skeleton_model):\n",
    "    temporal_data = []\n",
    "    # loop through all the points (0,22)\n",
    "    for i in range(len(chunks)):\n",
    "        prediction = []\n",
    "        confidence = []\n",
    "        data = chunks[i] # list of chunks in a given row\n",
    "        model_pred = skeleton_model.predict(processed[i])\n",
    "        print(\"Point: \"+ str(i) + \", Segments: \" + str(len(data)))\n",
    "        for j in range(len(data)):\n",
    "            seg_range = data[j] \n",
    "            lower = seg_range[0]\n",
    "            upper = seg_range[1]+1\n",
    "            seg_len = upper - lower\n",
    "            seg = model_pred[lower:upper]\n",
    "            \n",
    "            # get the predictions per frame\n",
    "            pred_frame = seg.argmax(axis=-1)\n",
    "            # adding up all the confident scores together\n",
    "            conf_sum = np.sum(seg, axis=0)/seg_len\n",
    "            # get the prediction with highest confident score\n",
    "            pred = conf_sum.argmax()\n",
    "            # get the highest confident score \n",
    "            conf = conf_sum.max()\n",
    "            # count how many times the prediction apppears per chunk \n",
    "            pred_count = np.sum(pred == pred_frame)\n",
    "            # get the percent of how many times it appears \n",
    "            pred_mode = pred_count/seg_len\n",
    "            # get the list of predictions in decending order \n",
    "            rank_pred = np.argsort(conf_sum.argsort())\n",
    "\n",
    "            # print(i, \": (\", j+1, \"/\", len(data), \"):\", pred ,\"[\", lower, \",\", upper, \"], (\", pred_count, \"/\", seg_len, \") =\", pred_mode, \",\", conf)\n",
    "            add_data = [i, rank_pred, seg_range, pred_count, seg_len, pred_mode, conf, conf_sum]\n",
    "            prediction.append(add_data)\n",
    "        temporal_data.append(prediction)\n",
    "    return temporal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_data = getTemporalData(chunks, skeleton_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTemporal(temporal_data, num_pts, num_fms):\n",
    "    temp_new = np.ones((num_pts,num_fms))\n",
    "    for i in range(len(temporal_data)):\n",
    "        for j in range(len(temporal_data[i])):\n",
    "            start = temporal_data[i][j][2][0]\n",
    "            end = temporal_data[i][j][2][1] + 1\n",
    "            for k in range(start, end):\n",
    "                temp_new[i][k] +=1\n",
    "    plt.imshow(temp_new, interpolation='nearest', aspect=\"auto\", cmap = \"Set3\")\n",
    "    plt.title(\"Full segments\")\n",
    "    plt.xlabel(\"Time (frames)\")\n",
    "    plt.ylabel(\"XYZ points\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkTemporal(temporal_data, num_pts, num_fms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSegmentation(temporal_data, num_pts):\n",
    "    temp_copy = temporal_data.copy()\n",
    "    temp_data = np.array(temp_copy, dtype=object)\n",
    "    segment_data = []\n",
    "\n",
    "    for i in range(20):\n",
    "        segment_data.append([])    \n",
    "\n",
    "    # putting the preditions into buckets\n",
    "    for i in range(num_pts):\n",
    "        index = temp_data[i]\n",
    "        for j in range(len(index)):\n",
    "            segment_data[index[j][1].argmax()].append(index[j])\n",
    "    for i in range(len(segment_data)):\n",
    "        print(\"Joint:\",i, \"Segment:\", len(segment_data[i]))\n",
    "    return segment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data = getSegmentation(temporal_data, num_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squish(chunk_data):\n",
    "    unprocessed_pts = []\n",
    "    processed_pts = []\n",
    "    # squish all the data points\n",
    "    for i in range(20): \n",
    "        numb_index = []\n",
    "        unprocessed = []\n",
    "        bounds = None\n",
    "        current = chunk_data[i]\n",
    "        cert = chunk_data[i]\n",
    "        sorted_current = sorted(current,key=lambda x: x[4])\n",
    "        \n",
    "        for j in reversed(range(len(sorted_current))):\n",
    "            input_index = sorted_current[j]\n",
    "            current_bounds = np.arange(input_index[2][0], input_index[2][1]+1).tolist()\n",
    "\n",
    "            if len(numb_index) == 0:\n",
    "                numb_index.append(input_index)\n",
    "                bounds = current_bounds\n",
    "            else:\n",
    "                same = list(set(current_bounds) & set(bounds)) # gets overlapping elements \n",
    "                # no overlapp between the two lists\n",
    "                if len(same) == 0:\n",
    "                    numb_index.append(input_index)\n",
    "                    bounds += current_bounds\n",
    "                else: #if there is overlap between the two lists\n",
    "                    if current_bounds[0] in bounds or current_bounds[-1] in bounds:\n",
    "                        unprocessed.append(input_index)\n",
    "                        pass\n",
    "                    elif current_bounds[0] in bounds:\n",
    "                        print(\"Lower bound inside current bounds\")\n",
    "                        pass\n",
    "                    elif current_bounds[-1] in bounds:\n",
    "                        print(\"Upper bound inside current bounds\")\n",
    "                        pass\n",
    "            bounds = sorted(bounds)\n",
    "        processed_pts.append(numb_index)\n",
    "        unprocessed_pts.append(unprocessed)\n",
    "\n",
    "    processed_pts = np.array(processed_pts, dtype = object)\n",
    "    unprocessed_pts = np.array(unprocessed_pts, dtype = object)\n",
    "    return processed_pts, unprocessed_pts\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pts, unprocessed_pts = squish(segment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictedSegments(time_pts, processed_pts, num_fms):\n",
    "    final_points = []\n",
    "    point_coord = np.ones((60,num_fms))*np.nan\n",
    "    error = False\n",
    "    for i in range(len(processed_pts)):\n",
    "        index = [i*3, i*3+1, i*3+2]\n",
    "        coord = [[],[],[]]\n",
    "\n",
    "        for j in range(len(processed_pts[i])):\n",
    "            seg_range = processed_pts[i][j]\n",
    "            org_index = seg_range[0]\n",
    "            start = seg_range[2][0]\n",
    "            end = seg_range[2][1]\n",
    "            index_xyz = [org_index*3, org_index*3+1, org_index*3+2]\n",
    "\n",
    "            for k in range(start, end):\n",
    "                point_coord[index[0]][k] = time_pts[index_xyz[0]][k]\n",
    "                point_coord[index[1]][k] = time_pts[index_xyz[1]][k]\n",
    "                point_coord[index[2]][k] = time_pts[index_xyz[2]][k]\n",
    "                if np.isnan(time_pts[index_xyz[0]][k]):\n",
    "                    error = True\n",
    "                    # print(\"big error\", seg_range)\n",
    "    if error:\n",
    "        print(\"BIG MISTAKE OCCURED\")\n",
    "    \n",
    "    # headpoints\n",
    "    headpts = time_pts[:3,:]\n",
    "    bodypt = point_coord[3:,:]\n",
    "    point_coord = np.concatenate((headpts,bodypt))\n",
    "\n",
    "    return point_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coord = predictedSegments(time_pts, processed_pts, num_fms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orginal_VS_processed(time_pts, point_coord):\n",
    "    plt.imshow(time_pts, interpolation='nearest', aspect=\"auto\")\n",
    "    plt.title(\"Original Full segments\")\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"XYZ points\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(point_coord, interpolation='nearest', aspect=\"auto\")\n",
    "    plt.title(\"Predicted Full segments\")\n",
    "    plt.xlabel(\"Time (frames)\")\n",
    "    plt.ylabel(\"XYZ points\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_VS_processed(time_pts, point_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "figure = point_coord.copy()#[:,::5]\n",
    "connections = [[0,1,2,0], [2,3,4,5], [3,6,7,5], [10, 11, 12, 3, 13, 14, 15], [18,17,8,5,9,16,19]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.set_title(\"Predicted Skeleton\")\n",
    "ax.set_xlabel(\"X axis\")\n",
    "ax.set_ylabel(\"Y axis\")\n",
    "ax.set_zlabel(\"Z axis\")\n",
    "\n",
    "def animfunc(t):\n",
    "    ax.clear()\n",
    "    \n",
    "    lines = []\n",
    "    frame = t\n",
    "    for conn in connections:\n",
    "        x = [3*i for i in conn]\n",
    "        y = [i+1 for i in x]\n",
    "        z = [i+2 for i in x]\n",
    "        lines.append(ax.scatter(figure[x, frame], figure[y,frame],  figure[z, frame], color='b', s =10))\n",
    "        lines.append(ax.plot(figure[x, frame], figure[y,frame],  figure[z, frame], linewidth=3))\n",
    "    return lines,\n",
    "        \n",
    "anim = FuncAnimation(fig, animfunc, frames=figure.shape[1], interval=10, repeat=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7fe8e863f55b74cc36bb0ed437f3bfb838b47acc1ca1a0dc10121f527a1e7d5c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
